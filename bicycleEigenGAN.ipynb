{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d54b09-7483-4d10-959d-10213f1c383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy\n",
    "import functools\n",
    "import traceback\n",
    "\n",
    "import imlib as im\n",
    "import numpy as np\n",
    "import pylib as py\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import tflib as tl\n",
    "import tfprob\n",
    "import tqdm\n",
    "\n",
    "import data\n",
    "import module\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                   param                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "py.arg('--img_dir', default='./data/img_celeba/aligned/align_size(572,572)_move(0.250,0.000)_face_factor(0.450)_jpg/data')\n",
    "py.arg('--load_size', type=int, default=256)\n",
    "py.arg('--crop_size', type=int, default=256)\n",
    "py.arg('--n_channels', type=int, choices=[1, 3], default=3)\n",
    "\n",
    "py.arg('--n_epochs', type=int, default=160)\n",
    "py.arg('--epoch_start_decay', type=int, default=160)\n",
    "py.arg('--batch_size', type=int, default=64)\n",
    "py.arg('--learning_rate', type=float, default=1e-4)\n",
    "py.arg('--beta_1', type=float, default=0.5)\n",
    "py.arg('--moving_average_decay', type=float, default=0.999)\n",
    "\n",
    "py.arg('--n_d', type=int, default=1)  # # d updates per g update\n",
    "py.arg('--adversarial_loss_mode', choices=['gan', 'hinge_v1', 'hinge_v2', 'lsgan', 'wgan'], default='hinge_v1')\n",
    "py.arg('--gradient_penalty_mode', choices=['none', '1-gp', '0-gp', 'lp'], default='0-gp')\n",
    "py.arg('--gradient_penalty_sample_mode', choices=['line', 'real', 'fake', 'real+fake', 'dragan', 'dragan_fake'], default='real')\n",
    "\n",
    "py.arg('--d_loss_weight_x_gan', type=float, default=1)\n",
    "py.arg('--d_loss_weight_x_gp', type=float, default=10)\n",
    "py.arg('--d_lazy_reg_period', type=int, default=3)\n",
    "\n",
    "py.arg('--g_loss_weight_x_gan', type=float, default=1)\n",
    "py.arg('--g_loss_weight_orth_loss', type=float, default=1)  # if 0, use Gramâ€“Schmidt orthogonalization (slower)\n",
    "\n",
    "py.arg('--d_attribute_loss_weight', type=float, default=1.0)\n",
    "py.arg('--g_attribute_loss_weight', type=float, default=10.0)\n",
    "py.arg('--g_reconstruction_loss_weight', type=float, default=100.0)\n",
    "\n",
    "py.arg('--weight_decay', type=float, default=0)\n",
    "\n",
    "py.arg('--z_dims', type=int, nargs='+', default=[6] * 6)\n",
    "py.arg('--eps_dim', type=int, default=512)\n",
    "\n",
    "py.arg('--n_samples', type=int, default=100)\n",
    "py.arg('--n_traversal', type=int, default=5)\n",
    "py.arg('--n_left_axis_point', type=int, default=10)\n",
    "py.arg('--truncation_threshold', type=int, default=1.5)\n",
    "\n",
    "py.arg('--sample_period', type=int, default=1000)\n",
    "py.arg('--traversal_period', type=int, default=2500)\n",
    "py.arg('--checkpoint_save_period', type=int, default=10000)\n",
    "\n",
    "py.arg('--experiment_name', default='default')\n",
    "#args = py.args()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3072c169-b6b9-463d-81f5-4e665fc3bbac",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = py.args([\"--experiment_name\",\"Eigen128_0526_unet_recon100\",\"--z_dims\",\"7\",\"7\",\"7\",\"7\",\"7\",\"--load_size\",\"128\",\"--crop_size\",\"128\"])\n",
    "#args = py.args([\"--experiment_name\",\"Eigen128_0602_unet_recon100\",\"--z_dims\",\"7\",\"7\",\"7\",\"7\",\"7\",\"--load_size\",\"128\",\"--crop_size\",\"128\"])\n",
    "\n",
    "\n",
    "#args = py.args([\"--experiment_name\",\"Eigen256_0524_unet_l\",\"--load_size\",\"256\",\"--crop_size\",\"256\",\"--batch_size\",\"32\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edd14d-b6ca-416a-82e5-3249686589be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import tflib as tl\n",
    "\n",
    "\n",
    "from tqdm.auto import tqdm, trange\n",
    "from pdb import set_trace\n",
    "    \n",
    "\n",
    "\n",
    "class DD(tl.Module):\n",
    "\n",
    "    def call(self,\n",
    "             x,\n",
    "             n_atts,\n",
    "             dim_10=4,\n",
    "             fc_dim=1024,\n",
    "             n_downsamplings=6,\n",
    "             weight_norm='none',\n",
    "             feature_norm='none',\n",
    "             act=tf.nn.leaky_relu,\n",
    "             training=True):\n",
    "        MAX_DIM = 512\n",
    "        nd = lambda size: min(int(2**(10 - np.log2(size)) * dim_10), MAX_DIM)\n",
    "\n",
    "        w_norm = tl.get_weight_norm(weight_norm, training)\n",
    "        conv = functools.partial(tl.conv2d, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "        fc = functools.partial(tl.fc, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "\n",
    "        f_norm = tl.get_feature_norm(feature_norm, training, updates_collections=None)\n",
    "        conv_norm_act = functools.partial(conv, normalizer_fn=f_norm, activation_fn=act)\n",
    "\n",
    "        \n",
    "        h = x\n",
    "        h = act(conv(h, nd(h.shape[1].value), 7, 1))\n",
    "        for i in range(n_downsamplings):\n",
    "            # h = conv_norm_act(h, nd(h.shape[1].value // 2), 4, 2)\n",
    "            h = conv_norm_act(h, nd(h.shape[1].value), 3, 1)\n",
    "            h = conv_norm_act(h, nd(h.shape[1].value // 2), 3, 2)\n",
    "\n",
    "            \n",
    "        h = conv_norm_act(h, nd(h.shape[1].value), 3, 1)\n",
    "        h = slim.flatten(h)\n",
    "        h = act(fc(h, min(fc_dim, MAX_DIM)))\n",
    "        logit_gan = fc(h, 1)\n",
    "        logit_att = fc(h, n_atts)\n",
    "\n",
    "        return logit_gan, logit_att\n",
    "    \n",
    "    \n",
    "    \n",
    "class UNetGenc(tl.Module):\n",
    "\n",
    "    def call(self,\n",
    "             x,\n",
    "             dim_10=4,\n",
    "             n_channels=3,\n",
    "             n_downsamplings=6,\n",
    "             weight_norm='none',\n",
    "             feature_norm='none',\n",
    "             act=tf.nn.leaky_relu,\n",
    "             training=True):\n",
    "        MAX_DIM = 512\n",
    "        nd = lambda size: min(int(2**(10 - np.log2(size)) * dim_10), MAX_DIM)\n",
    "\n",
    "        w_norm = tl.get_weight_norm(weight_norm, training)\n",
    "        conv = functools.partial(tl.conv2d, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "        fc = functools.partial(tl.fc, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "\n",
    "        f_norm = tl.get_feature_norm(feature_norm, training, updates_collections=None)\n",
    "        conv_norm_act = functools.partial(conv, normalizer_fn=f_norm, activation_fn=act)\n",
    "\n",
    "        hiddenLayer = []\n",
    "        \n",
    "        h = x\n",
    "        h = act(conv(h, nd(h.shape[1].value), 7, 1))\n",
    "        for i in range(n_downsamplings):\n",
    "            # h = conv_norm_act(h, nd(h.shape[1].value // 2), 4, 2)\n",
    "            h = conv_norm_act(h, nd(h.shape[1].value), 3, 1)\n",
    "            hiddenLayer.append(h)\n",
    "\n",
    "            h = conv_norm_act(h, nd(h.shape[1].value // 2), 3, 2)\n",
    "            hiddenLayer.append(h)\n",
    "\n",
    "        return hiddenLayer\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "class UNetGdec(tl.Module):\n",
    "\n",
    "    def call(self,\n",
    "             zs,\n",
    "             eps,\n",
    "             dim_10=4,\n",
    "             n_channels=3,\n",
    "             weight_norm='none',\n",
    "             feature_norm='none',\n",
    "             act=tf.nn.leaky_relu,\n",
    "             use_gram_schmidt=True,\n",
    "             training=True,\n",
    "            shortcut_layers=1):\n",
    "        MAX_DIM = 512\n",
    "        nd = lambda size: min(int(2**(10 - np.log2(size)) * dim_10), MAX_DIM)\n",
    "\n",
    "        w_norm = tl.get_weight_norm(weight_norm, training)\n",
    "        transposed_w_norm = tl.get_weight_norm(weight_norm, training, transposed=True)\n",
    "        fc = functools.partial(tl.fc, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "        conv = functools.partial(tl.conv2d, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "        dconv = functools.partial(tl.dconv2d, weights_initializer=tl.get_initializer(act), weights_normalizer_fn=transposed_w_norm, weights_regularizer=slim.l2_regularizer(1.0))\n",
    "        f_norm = tl.get_feature_norm(feature_norm, training, updates_collections=None)\n",
    "        f_norm = (lambda x: x) if f_norm is None else f_norm\n",
    "\n",
    "        def orthogonal_regularizer(U):\n",
    "            with tf.name_scope('orthogonal_regularizer'):\n",
    "                U = tf.reshape(U, [-1, U.shape[-1]])\n",
    "                orth = tf.matmul(tf.transpose(U), U)\n",
    "                tf.add_to_collections(['orth'], orth)\n",
    "                return 0.5 * tf.reduce_sum((orth - tf.eye(U.shape[-1].value)) ** 2)\n",
    "\n",
    "            \n",
    "        h=eps[-1]\n",
    "        \n",
    "        \n",
    "        for i, z in enumerate(zs):\n",
    "            height = width = 4 * 2 ** i\n",
    "\n",
    "            U = tf.get_variable('U_%d' % i,\n",
    "                                shape=[height, width, nd(height), z.shape[-1]],\n",
    "                                initializer=tf.initializers.orthogonal(),\n",
    "                                regularizer=orthogonal_regularizer,\n",
    "                                trainable=True)\n",
    "            if use_gram_schmidt:\n",
    "                U = tf.transpose(tf.reshape(U, [-1, U.shape[-1]]))\n",
    "                U = tl.gram_schmidt(U)\n",
    "                U = tf.reshape(tf.transpose(U), [height, width, nd(height), z.shape[-1]])\n",
    "\n",
    "            L = tf.get_variable('L_%d' % i,\n",
    "                                shape=[z.shape[-1]],\n",
    "                                initializer=tf.initializers.constant([3 * i for i in range(z.shape[-1], 0, -1)]),\n",
    "                                trainable=True)\n",
    "\n",
    "            mu = tf.get_variable('mu_%d' % i,\n",
    "                                 shape=[height, width, nd(height)],\n",
    "                                 initializer=tf.initializers.zeros(),\n",
    "                                 trainable=True)\n",
    "\n",
    "            h_ = tf.reduce_sum(U[None, ...] * (L[None, :] * z)[:, None, None, None, :], axis=-1) + mu[None, ...]\n",
    "\n",
    "            h_1 = dconv(h_, nd(height), 1, 1)\n",
    "            \n",
    "            if shortcut_layers > i:\n",
    "                h_2 = dconv(h_, nd(height * 2)*2, 3, 2)\n",
    "            else:\n",
    "                h_2 = dconv(h_, nd(height * 2), 3, 2)\n",
    "            \n",
    "            \n",
    "            #deconv1\n",
    "            h=act(f_norm(h + h_1))\n",
    "            #if shortcut_layers > i:\n",
    "            #    h = tl.tile_concat([h, eps[-1 - 2*i]])\n",
    "            h = dconv(h, nd(height * 2), 3, 2)\n",
    "\n",
    "            \n",
    "            \n",
    "            if shortcut_layers > i:\n",
    "                h = tl.tile_concat([h, eps[-2 - 2*i]])\n",
    "            #deconv2\n",
    "            h=act(f_norm(h + h_2))\n",
    "            h = dconv(h, nd(height * 2), 3, 1)\n",
    "            \n",
    "        x = tf.tanh(conv(act(h), n_channels, 7, 1))\n",
    "\n",
    "        return x    \n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c94e30e-8414-4803-9c67-3b4496c01b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylib as py\n",
    "import tensorflow as tf\n",
    "import tflib as tl\n",
    "\n",
    "\n",
    "def make_dataset(img_paths,\n",
    "                 batch_size,\n",
    "                 load_size=286,\n",
    "                 crop_size=256,\n",
    "                 n_channels=3,\n",
    "                 training=True,\n",
    "                 drop_remainder=True,\n",
    "                 shuffle=True,\n",
    "                 repeat=1):\n",
    "    \n",
    "\n",
    "    if shuffle:\n",
    "        img_paths = np.random.permutation(img_paths)\n",
    "\n",
    "    if training:\n",
    "        def _map_fn(img):\n",
    "            if n_channels == 1:\n",
    "                img = tf.image.rgb_to_grayscale(img)\n",
    "            img = tf.image.resize(img, [load_size, load_size])\n",
    "            img = tf.image.random_flip_left_right(img)\n",
    "            img = tl.center_crop(img, size=crop_size)\n",
    "            # img = tf.image.random_crop(img, [crop_size, crop_size, n_channels])\n",
    "            img = tf.clip_by_value(img, 0, 255) / 127.5 - 1\n",
    "            return img\n",
    "    else:\n",
    "        def _map_fn(img):\n",
    "            if n_channels == 1:\n",
    "                img = tf.image.rgb_to_grayscale(img)\n",
    "            img = tf.image.resize(img, [load_size, load_size])\n",
    "            img = tl.center_crop(img, size=crop_size)\n",
    "            img = tf.clip_by_value(img, 0, 255) / 127.5 - 1\n",
    "            return img\n",
    "\n",
    "    dataset = tl.disk_image_batch_dataset(img_paths,\n",
    "                                          batch_size,\n",
    "                                          drop_remainder=drop_remainder,\n",
    "                                          map_fn=_map_fn,\n",
    "                                          shuffle=shuffle,\n",
    "                                          repeat=repeat)\n",
    "\n",
    "    if drop_remainder:\n",
    "        len_dataset = len(img_paths) // batch_size\n",
    "    else:\n",
    "        len_dataset = int(np.ceil(len(img_paths) / batch_size))\n",
    "\n",
    "    return dataset, len_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d86d80-02d6-409d-9e6f-1801718fe58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# check\n",
    "assert np.log2(args.crop_size / 4) == len(args.z_dims)\n",
    "\n",
    "# output_dir\n",
    "output_dir = py.join('output', args.experiment_name)\n",
    "py.mkdir(output_dir)\n",
    "\n",
    "# save settings\n",
    "py.args_to_yaml(py.join(output_dir, 'settings.yml'), args)\n",
    "\n",
    "sess = tl.session()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                    data                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "\n",
    "img_paths=sorted(py.glob(args.img_dir, '*'))\n",
    "img_paths_train = img_paths[:int(len(img_paths)*0.95)]\n",
    "img_paths_test = img_paths[int(len(img_paths)*0.95):]\n",
    "\n",
    "\n",
    "train_dataset, len_train_dataset = make_dataset(img_paths_train, args.batch_size, load_size=args.load_size, crop_size=args.crop_size, n_channels=args.n_channels, repeat=None)\n",
    "train_iter = train_dataset.make_one_shot_iterator()\n",
    "\n",
    "val_dataset, len_val_dataset = make_dataset(img_paths_test, max(args.n_traversal, args.n_samples), load_size=args.load_size, crop_size=args.crop_size, n_channels=args.n_channels, shuffle=False,repeat=None,training=False)\n",
    "val_iter = val_dataset.make_one_shot_iterator()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                   model                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "#D = functools.partial(module.D(scope='D'), n_downsamplings=len(args.z_dims))\n",
    "D = functools.partial(DD(scope='D'), n_atts=sum(args.z_dims), n_downsamplings=len(args.z_dims))\n",
    "#G = functools.partial(module.G(scope='G'), n_channels=args.n_channels, use_gram_schmidt=args.g_loss_weight_orth_loss == 0)\n",
    "Genc = functools.partial(UNetGenc(scope='Gdec'), n_channels=args.n_channels, n_downsamplings=len(args.z_dims))\n",
    "Gdec = functools.partial(UNetGdec(scope='Genc'), n_channels=args.n_channels, use_gram_schmidt=args.g_loss_weight_orth_loss == 0)\n",
    "G_test = functools.partial(UNetGdec(scope='G_test'), n_channels=args.n_channels, use_gram_schmidt=args.g_loss_weight_orth_loss == 0, training=False)\n",
    "\n",
    "# exponential moving average\n",
    "G_ema = tf.train.ExponentialMovingAverage(decay=args.moving_average_decay, name='G_ema')\n",
    "\n",
    "# loss function\n",
    "d_loss_fn, g_loss_fn = tfprob.get_adversarial_losses_fn(args.adversarial_loss_mode)\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# =                                   graph                                    =\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "def D_train_graph():\n",
    "    # ======================================\n",
    "    # =               graph                =\n",
    "    # ======================================\n",
    "\n",
    "    # placeholders & inputs\n",
    "    lr = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "    x_r = train_iter.get_next()\n",
    "    zs = [tf.random.normal([args.batch_size, z_dim]) for z_dim in args.z_dims]\n",
    "    eps = tf.random.normal([args.batch_size, args.eps_dim])\n",
    "\n",
    "    # counter\n",
    "    step_cnt, _ = tl.counter()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=args.beta_1)\n",
    "\n",
    "    def graph_per_gpu(x_r, zs, eps):\n",
    "\n",
    "        # generate\n",
    "        eps=Genc(x_r)\n",
    "        x_f=Gdec(zs,eps)\n",
    "        \n",
    "        # discriminate\n",
    "        x_r_logit,_ = D(x_r)\n",
    "        x_f_logit,x_f_logit_att = D(x_f)\n",
    "\n",
    "        # loss\n",
    "        x_r_loss, x_f_loss = d_loss_fn(x_r_logit, x_f_logit)\n",
    "        x_gp = tf.cond(tf.equal(step_cnt % args.d_lazy_reg_period, 0),\n",
    "                       lambda: tfprob.gradient_penalty(D, x_r, x_f, args.gradient_penalty_mode, args.gradient_penalty_sample_mode) * args.d_lazy_reg_period,\n",
    "                       lambda: tf.constant(0.0))\n",
    "        if args.d_loss_weight_x_gp == 0:\n",
    "            x_gp = tf.constant(0.0)\n",
    "\n",
    "        reg_loss = tf.reduce_sum(D.func.reg_losses)\n",
    "\n",
    "       \n",
    "        zs_flatten = tf.concat(zs,axis=1)\n",
    "        xb__loss_att=tf.losses.mean_squared_error(zs_flatten, x_f_logit_att)  \n",
    "        \n",
    "        \n",
    "        loss = (\n",
    "            (x_r_loss + x_f_loss) * args.d_loss_weight_x_gan +\n",
    "            x_gp * args.d_loss_weight_x_gp +\n",
    "            reg_loss * args.weight_decay +\n",
    "            xb__loss_att * args.d_attribute_loss_weight\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        # optim\n",
    "        grads = optimizer.compute_gradients(loss, var_list=D.func.trainable_variables)\n",
    "\n",
    "        return grads, x_r_loss, x_f_loss, x_gp, reg_loss\n",
    "\n",
    "    split_grads, split_x_r_loss, split_x_f_loss, split_x_gp, split_reg_loss = zip(*tl.parellel_run(tl.gpus(), graph_per_gpu, tl.split_nest((x_r, zs, eps), len(tl.gpus()))))\n",
    "    # split_grads, split_x_r_loss, split_x_f_loss, split_x_gp, split_reg_loss = zip(*tl.parellel_run(['cpu:0'], graph_per_gpu, tl.split_nest((x_r, zs, eps), 1)))\n",
    "    grads = tl.average_gradients(split_grads)\n",
    "    x_r_loss, x_f_loss, x_gp, reg_loss = [tf.reduce_mean(t) for t in [split_x_r_loss, split_x_f_loss, split_x_gp, split_reg_loss]]\n",
    "\n",
    "    step = optimizer.apply_gradients(grads, global_step=step_cnt)\n",
    "\n",
    "    # summary\n",
    "    summary = tl.create_summary_statistic_v2(\n",
    "        {'x_gan_loss': x_r_loss + x_f_loss,\n",
    "         'x_gp': x_gp,\n",
    "         'reg_loss': reg_loss,\n",
    "         'lr': lr},\n",
    "        './output/%s/summaries/D' % args.experiment_name,\n",
    "        step=step_cnt,\n",
    "        n_steps_per_record=10,\n",
    "        name='D'\n",
    "    )\n",
    "\n",
    "    # ======================================\n",
    "    # =            run function            =\n",
    "    # ======================================\n",
    "\n",
    "    def run(**pl_ipts):\n",
    "        for _ in range(args.n_d):\n",
    "            sess.run([step, summary], feed_dict={lr: pl_ipts['lr']})\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def G_train_graph():\n",
    "    # ======================================\n",
    "    # =               graph                =\n",
    "    # ======================================\n",
    "\n",
    "    # placeholders & inputs\n",
    "    lr = tf.placeholder(dtype=tf.float32, shape=[])\n",
    "    zs = [tf.random.normal([args.batch_size, z_dim]) for z_dim in args.z_dims]\n",
    "    eps = tf.random.normal([args.batch_size, args.eps_dim])\n",
    "    x_r = train_iter.get_next()\n",
    "    \n",
    "    # counter\n",
    "    step_cnt, _ = tl.counter()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr, beta1=args.beta_1)\n",
    "\n",
    "    def graph_per_gpu(zs, eps):\n",
    "        # generate\n",
    "        _,zs_a = D(x_r)\n",
    "        zs_a=tf.split(zs_a, len(args.z_dims), axis=1)\n",
    "        \n",
    "        eps=Genc(x_r)\n",
    "        x_f=Gdec(zs,eps)\n",
    "        x_a=Gdec(zs_a,eps)\n",
    "        \n",
    "        \n",
    "        # discriminate\n",
    "        x_f_logit,xb__logit_att = D(x_f)\n",
    "\n",
    "        # loss\n",
    "        x_f_loss = g_loss_fn(x_f_logit)\n",
    "        orth_loss = tf.reduce_sum(tl.tensors_filter(Gdec.func.reg_losses, 'orthogonal_regularizer'))\n",
    "        reg_loss_Gdec = tf.reduce_sum(tl.tensors_filter(Gdec.func.reg_losses, 'l2_regularizer'))\n",
    "        reg_loss_Genc = tf.reduce_sum(tl.tensors_filter(Genc.func.reg_losses, 'l2_regularizer'))\n",
    "        reg_loss=reg_loss_Gdec+reg_loss_Genc\n",
    "                \n",
    "        zs_flatten = tf.concat(zs,axis=1)\n",
    "        xb__loss_att= xb__loss_att=tf.losses.mean_squared_error(zs_flatten, xb__logit_att)  \n",
    "        xa__loss_rec = tf.losses.absolute_difference(x_r, x_a)\n",
    "        \n",
    "        loss = (\n",
    "            x_f_loss * args.g_loss_weight_x_gan +\n",
    "            orth_loss * args.g_loss_weight_orth_loss +\n",
    "            reg_loss * args.weight_decay +\n",
    "            xb__loss_att * args.g_attribute_loss_weight +\n",
    "            xa__loss_rec *  args.g_reconstruction_loss_weight \n",
    "        )\n",
    "\n",
    "        \n",
    "        # optim\n",
    "        #grads = optimizer.compute_gradients(loss, var_list=G.func.trainable_variables)\n",
    "        grads = optimizer.compute_gradients(loss, var_list=Genc.func.trainable_variables+Gdec.func.trainable_variables)\n",
    "\n",
    "        return grads, x_f_loss, orth_loss, reg_loss\n",
    "\n",
    "    split_grads, split_x_f_loss, split_orth_loss, split_reg_loss = zip(*tl.parellel_run(tl.gpus(), graph_per_gpu, tl.split_nest((zs, eps), len(tl.gpus()))))\n",
    "    # split_grads, split_x_f_loss, split_orth_loss, split_reg_loss = zip(*tl.parellel_run(['cpu:0'], graph_per_gpu, tl.split_nest((zs, eps), 1)))\n",
    "    grads = tl.average_gradients(split_grads)\n",
    "    x_f_loss, orth_loss, reg_loss = [tf.reduce_mean(t) for t in [split_x_f_loss, split_orth_loss, split_reg_loss]]\n",
    "\n",
    "    step = optimizer.apply_gradients(grads, global_step=step_cnt)\n",
    "\n",
    "    # moving average\n",
    "    with tf.control_dependencies([step]):\n",
    "        step = G_ema.apply(Gdec.func.trainable_variables)\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "    # summary\n",
    "    summary_dict = {'x_f_loss': x_f_loss,\n",
    "                    'orth_loss': orth_loss,\n",
    "                    'reg_loss': reg_loss}\n",
    "    summary_dict.update({'L_%d' % i: t for i, t in enumerate(tl.tensors_filter(Genc.func.trainable_variables+Gdec.func.trainable_variables, 'L'))})\n",
    "    summary_loss = tl.create_summary_statistic_v2(\n",
    "        summary_dict,\n",
    "        './output/%s/summaries/G' % args.experiment_name,\n",
    "        step=step_cnt,\n",
    "        n_steps_per_record=10,\n",
    "        name='G_loss'\n",
    "    )\n",
    "\n",
    "    summary_image = tl.create_summary_image_v2(\n",
    "        {'orth_U_%d' % i: t[None, :, :, None] for i, t in enumerate(tf.get_collection('orth', Gdec.func.scope + '/'))},\n",
    "        './output/%s/summaries/G' % args.experiment_name,\n",
    "        step=step_cnt,\n",
    "        n_steps_per_record=10,\n",
    "        name='G_image'\n",
    "    )\n",
    "\n",
    "    # ======================================\n",
    "    # =             model size             =\n",
    "    # ======================================\n",
    "\n",
    "    n_params, n_bytes = tl.count_parameters(Genc.func.trainable_variables+Gdec.func.trainable_variables)\n",
    "    print('Model Size: n_parameters = %d = %.2fMB' % (n_params, n_bytes / 1024 / 1024))\n",
    "\n",
    "    # ======================================\n",
    "    # =            run function            =\n",
    "    # ======================================\n",
    "\n",
    "    def run(**pl_ipts):\n",
    "        sess.run([step, summary_loss, summary_image], feed_dict={lr: pl_ipts['lr']})\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def sample_graph():\n",
    "\n",
    "    # ======================================\n",
    "    # =               graph                =\n",
    "    # ======================================\n",
    "\n",
    "    # placeholders & inputs\n",
    "    zs = [tl.truncated_normal([args.n_samples, z_dim], minval=-args.truncation_threshold, maxval=args.truncation_threshold) for z_dim in args.z_dims]\n",
    "    eps = tl.truncated_normal([args.n_samples, args.eps_dim], minval=-args.truncation_threshold, maxval=args.truncation_threshold)\n",
    "    xa = tf.placeholder(tf.float32, shape=[None, args.crop_size, args.crop_size, 3])\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    # generate\n",
    "    x_r = val_iter.get_next()\n",
    "    x_f = G_test(zs,Genc(xa, training=False), training=False)\n",
    "    \n",
    "    # ======================================\n",
    "    # =            run function            =\n",
    "    # ======================================\n",
    "\n",
    "    save_dir = './output/%s/samples_training/sample' % (args.experiment_name)\n",
    "    py.mkdir(save_dir)\n",
    "\n",
    "    def run(epoch, iter):\n",
    "        xa_ipt = sess.run(x_r)\n",
    "        \n",
    "        x_f_opt = sess.run(x_f, feed_dict={xa: xa_ipt[:args.n_samples]})\n",
    "        sample = im.immerge(x_f_opt, n_rows=int(args.n_samples ** 0.5))\n",
    "        im.imwrite(sample, '%s/Epoch-%d_Iter-%d.jpg' % (save_dir, epoch, iter))\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def traversal_graph():\n",
    "\n",
    "    # ======================================\n",
    "    # =               graph                =\n",
    "    # ======================================\n",
    "\n",
    "    # placeholders & inputs\n",
    "    zs = [tf.placeholder(dtype=tf.float32, shape=[args.n_traversal, z_dim]) for z_dim in args.z_dims]\n",
    "    eps = tf.placeholder(dtype=tf.float32, shape=[args.n_traversal, args.eps_dim])\n",
    "    x = tf.placeholder(tf.float32, shape=[None, args.crop_size, args.crop_size, 3])\n",
    "    \n",
    "    # generate\n",
    "    x_r = val_iter.get_next()\n",
    "    _,x_r_zs=D(x, training=False)\n",
    "    x_r_zs=tf.split(x_r_zs, len(args.z_dims), axis=1)\n",
    "    \n",
    "    \n",
    "    x_f = G_test(zs,Genc(x, training=False), training=False)\n",
    "    \n",
    "    \n",
    "    # ======================================\n",
    "    # =            run function            =\n",
    "    # ======================================\n",
    "\n",
    "    save_dir = './output/%s/samples_training/traversal' % (args.experiment_name)\n",
    "    py.mkdir(save_dir)\n",
    "\n",
    "    def run(epoch, iter):\n",
    "        x_r_input = sess.run(x_r)\n",
    "        x_r_input=x_r_input[:args.n_traversal]\n",
    "        x_r_zs_input=sess.run(x_r_zs, feed_dict={x:x_r_input})\n",
    "        x_r_zs_input=np.array(x_r_zs_input)\n",
    "        feed_dict = {z: z_ipt for z, z_ipt in zip(zs, x_r_zs_input)}\n",
    "        feed_dict.update({x: x_r_input})\n",
    "        \n",
    "        x_f_recon= sess.run(x_f,feed_dict=feed_dict)\n",
    "    \n",
    "        zs_ipt_fixed=x_r_zs_input\n",
    "        #zs_ipt_fixed = [scipy.stats.truncnorm.rvs(-args.truncation_threshold, args.truncation_threshold, size=[args.n_traversal, z_dim]) for z_dim in args.z_dims]\n",
    "        #eps_ipt = scipy.stats.truncnorm.rvs(-args.truncation_threshold, args.truncation_threshold, size=[args.n_traversal, args.eps_dim])\n",
    "        # set the first sample as the \"mode\"\n",
    "        #for l in range(len(args.z_dims)):\n",
    "        #    zs_ipt_fixed[l][0, ...] = 0.0\n",
    "        #eps_ipt[0, ...] = 0.0\n",
    "        \n",
    "\n",
    "        L_opt = sess.run(tl.tensors_filter(G_test.func.variables, 'L'))\n",
    "        for l in range(len(args.z_dims)):\n",
    "            for j, i in enumerate(np.argsort(np.abs(L_opt[l]))[::-1]):\n",
    "                x_f_opts = [x_r_input,x_f_recon]\n",
    "                \n",
    "                \n",
    "                \n",
    "                vals = np.linspace(-4.5, 4.5, args.n_left_axis_point * 2 + 1)\n",
    "                \n",
    "                for v in vals:\n",
    "                    zs_ipt = copy.deepcopy(zs_ipt_fixed)\n",
    "                    zs_ipt[l][:, i] = v\n",
    "                    feed_dict = {z: z_ipt for z, z_ipt in zip(zs, zs_ipt)}\n",
    "                    feed_dict.update({x: x_r_input})\n",
    "                    x_f_opt = sess.run(x_f, feed_dict=feed_dict)\n",
    "                    x_f_opts.append(x_f_opt)\n",
    "\n",
    "                sample = im.immerge(np.concatenate(x_f_opts, axis=2), n_rows=args.n_traversal)\n",
    "                im.imwrite(sample, '%s/Epoch-%d_Iter-%d_Traversal-%d-%d-%.3f-%d.jpg' % (save_dir, epoch, iter, l, j, np.abs(L_opt[l][i]), i))\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "def clone_graph():\n",
    "    # ======================================\n",
    "    # =               graph                =\n",
    "    # ======================================\n",
    "\n",
    "    clone_tr = G_test.func.clone_from_vars(tl.tensors_filter(tl.global_variables(), 'G_ema'), var_type='trainable')\n",
    "    clone_non = G_test.func.clone_from_module(Gdec.func, var_type='nontrainable')\n",
    "\n",
    "    # ======================================\n",
    "    # =            run function            =\n",
    "    # ======================================\n",
    "\n",
    "    def run(**pl_ipts):\n",
    "        sess.run([clone_tr, clone_non])\n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "d_train_step = D_train_graph()\n",
    "g_train_step = G_train_graph()\n",
    "sample = sample_graph()\n",
    "traversal = traversal_graph()\n",
    "clone = clone_graph()\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "# ==============================================================================\n",
    "# =                                   train                                    =\n",
    "# ==============================================================================\n",
    "\n",
    "# init\n",
    "checkpoint, step_cnt, update_cnt = tl.init(py.join(output_dir, 'checkpoints'), checkpoint_max_to_keep=1, session=sess)\n",
    "\n",
    "# learning rate schedule\n",
    "lr_fn = tl.LinearDecayLR(args.learning_rate, args.n_epochs, args.epoch_start_decay)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b369da7c-d9bd-43f5-b200-8ec22b37ed3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# train\n",
    "try:\n",
    "    for ep in trange(args.n_epochs, desc='Epoch Loop'):\n",
    "        # learning rate\n",
    "        lr_ipt = lr_fn(ep)\n",
    "\n",
    "        for it in trange(len_train_dataset // (args.n_d + 1), desc='Inner Epoch Loop'):\n",
    "            if it + ep * (len_train_dataset // (args.n_d + 1)) < sess.run(step_cnt):\n",
    "                continue\n",
    "            step = sess.run(update_cnt)\n",
    "\n",
    "            # train D\n",
    "            d_train_step(lr=lr_ipt)\n",
    "            # train G\n",
    "            g_train_step(lr=lr_ipt)\n",
    "\n",
    "            # save\n",
    "            if step % args.checkpoint_save_period == 0:\n",
    "                checkpoint.save(step, session=sess)\n",
    "\n",
    "            # sample\n",
    "            if step % args.sample_period == 0 :\n",
    "                clone()\n",
    "                sample(ep, it)\n",
    "            if step % args.traversal_period == 0 :\n",
    "                clone()\n",
    "                traversal(ep, it)\n",
    "except Exception:\n",
    "    traceback.print_exc()\n",
    "finally:\n",
    "    clone()\n",
    "    sample(ep, it)\n",
    "    traversal(ep, it)\n",
    "    checkpoint.save(step, session=sess)\n",
    "    sess.close()\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4263e6-2a6f-41f5-bcc6-c7ee5ea71e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#display sample\n",
    "\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from imlib import dtype\n",
    "\n",
    "\n",
    "\n",
    "def display_sample():\n",
    "\n",
    "    # ======================================\n",
    "    # =               graph                =\n",
    "    # ======================================\n",
    "\n",
    "    \n",
    "    # placeholders & inputs\n",
    "\n",
    "    zs = [tl.truncated_normal([args.n_samples, z_dim], minval=-args.truncation_threshold, maxval=args.truncation_threshold) for z_dim in args.z_dims]\n",
    "    eps = tl.truncated_normal([args.n_samples, args.eps_dim], minval=-args.truncation_threshold, maxval=args.truncation_threshold)\n",
    "    xa = tf.placeholder(tf.float32, shape=[None, args.crop_size, args.crop_size, 3])\n",
    "\n",
    "    # generate\n",
    "    x_r = val_iter.get_next()\n",
    "    _,x_r_zs=D(xa, training=False)\n",
    "    x_r_zs=tf.split(x_r_zs, len(args.z_dims), axis=1)\n",
    "    \n",
    "    x_f_rand = G_test(zs,Genc(xa, training=False), training=False)\n",
    "    x_f_recon = G_test(x_r_zs,Genc(xa, training=False), training=False)\n",
    "\n",
    "\n",
    "    \n",
    "    # ======================================\n",
    "    # =            run function            =\n",
    "    # ======================================\n",
    "    \n",
    "    def run():\n",
    "        xa_ipt = sess.run(x_r)[:args.n_samples]\n",
    "        x_f_opt_rand = sess.run(x_f_rand,  feed_dict={xa: xa_ipt})\n",
    "        x_f_opt_recon = sess.run(x_f_recon,  feed_dict={xa: xa_ipt})\n",
    "\n",
    "\n",
    "        \n",
    "        img=Image.fromarray(dtype.im2uint(xa_ipt[0]))\n",
    "        display(img)\n",
    "        img=Image.fromarray(dtype.im2uint(x_f_opt_recon[0]))\n",
    "        display(img)\n",
    "        img=Image.fromarray(dtype.im2uint(x_f_opt_rand[0]))\n",
    "        display(img)\n",
    "\n",
    "        \n",
    "\n",
    "    return run\n",
    "\n",
    "\n",
    "display_sample_func=display_sample()\n",
    "display_sample_func()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b4d881-7ef4-43dc-b594-47537c1e1db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
